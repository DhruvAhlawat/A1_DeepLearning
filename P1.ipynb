{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "import glob\n",
    "import os\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing the ResNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, n):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, stride=1, padding=1, device=device); # 16 filters, kernel size 3x3. output shape is same as input shape due to padding = 1\n",
    "        self.n = n;\n",
    "        #Now we have the next 3 blocks of ResNet. \n",
    "        # The first n blocks have 2n filters of size 3x3, and 16 filters, with residual connection between each 2 consecutive filters.\n",
    "        self.res16 = [];\n",
    "        for i in range(2*n): #2n channels of 16 filters each. Need to have residual connections between them too.\n",
    "            self.res16.append(nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, device= device)); \n",
    "\n",
    "        self.res16_32_1x1 = nn.Conv2d(16, 32, kernel_size=1, stride=2, padding=0, device=device); #1x1 convolution to increase the number of filters to 32, and halve the feature map size from 256x256 to 128x128.\n",
    "        self.res32 = [nn.Conv2d(16,32, kernel_size=3, stride=2, padding=1, device=device)]; #Halves the feature map size from 256x256 to 128x128, while increasing filters to 32\n",
    "        for i in range(2*n-1):\n",
    "            self.res32.append(nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, device=device));\n",
    "\n",
    "        self.res32_64_1x1 = nn.Conv2d(32, 64, kernel_size=1, stride=2, padding=0, device=device); #1x1 convolution to increase the number of filters to 64, and halve the feature map size from 128x128 to 64x64.\n",
    "        self.res64 = [nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, device=device)]; #Halves the feature map size from 128x128 to 64x64, while increasing filters to 64\n",
    "        for i in range(2*n-1):\n",
    "            self.res64.append(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, device=device));\n",
    "\n",
    "        self.final_mean_pool = nn.AvgPool2d(kernel_size=64, stride=1); #Average pooling to get the mean of the 64x64 feature map\n",
    "        self.fc = nn.Linear(64, num_classes, device=device); #Fully connected layer to output the class scores\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)); #First Convolutional layer.\n",
    "        #Now we have the next 3 blocks of ResNet.\n",
    "        #For the first n blocks. \n",
    "        residual = torch.clone(x).to(device); \n",
    "        for i in range(0,2*self.n-1,2): #with a step of 2, so we pass over each residual connection.\n",
    "            x = F.relu(self.res16[i](x));\n",
    "            x = self.res16[i+1](x)\n",
    "            x += residual; #We add the residual conneciton before passing it to the next layer.\n",
    "            x = F.relu(x); #The output is complete here.\n",
    "            residual = torch.clone(x).to(device); \n",
    "        return x;\n",
    "        #Now we have a residual of shape 16x256x256, we need to pass it through to the next layer of 32x128x128.\n",
    "        residual = self.res16_32_1x1(residual); #to match the dimensions of the next layer.\n",
    "        for i in range(0,2*self.n-1,2):\n",
    "            x = F.relu(self.res32[0](x));\n",
    "            x = self.res32[i+1](x) #the output is of 32x128x128 here.\n",
    "            x += residual; \n",
    "            x = F.relu(x);\n",
    "            residual = torch.clone(x).to(device); \n",
    "\n",
    "        residual = self.res32_64_1x1(residual); #to match the dimensions of the next layer.\n",
    "        for i in range(0,2*self.n-1,2):\n",
    "            x = F.relu(self.res64[0](x));\n",
    "            x = self.res64[i+1](x) #the output is of 64x64x64 here.\n",
    "            x += residual;\n",
    "            x = F.relu(x);\n",
    "            residual = torch.clone(x).to(device); \n",
    "        print(\"out_shape: \", x.shape); #The output shape is 64x64x64\n",
    "        x = self.final_mean_pool(x); #Average pooling to get the mean of the 64x64 feature map\n",
    "        print(\"after avgpool:\" ,x.shape);\n",
    "        return x;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bird_dataset(Dataset):\n",
    "    def __init__(self, datapath): #Either test, train, or val datafolder.\n",
    "        self.datapath = datapath;\n",
    "        folder_list = glob.glob(datapath + \"/*\");\n",
    "        self.data = [];\n",
    "        self.labels = set();\n",
    "        for folder in folder_list:\n",
    "            label = os.path.basename(folder); #gets the last name of the folder, which is the label.\n",
    "            self.labels.add(label);\n",
    "            file_list = glob.glob(folder + \"/*\");\n",
    "            for file in file_list:\n",
    "                self.data.append((file, label));\n",
    "        self.labels = list(self.labels);\n",
    "        self.label_to_index = {label: i for i, label in enumerate(self.labels)};\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data);\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx];\n",
    "        img = read_image(img_path)\n",
    "        img = img/255;\n",
    "        # print(img);\n",
    "        # img = transforms.ToTensor()(img); #converts the image to a tensor, but read_image already does this.\n",
    "        label = self.label_to_index[label]; #using labels as indices for the classes, instead of names.\n",
    "        return img, label;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters for the network.32\n",
    "num_classes = 25; \n",
    "n = 2; #6n + 2 layers.\n",
    "in_channels = 3; #RGB images.\n",
    "batch_size = 1; #Probably wont run on my laptop with just 4GB of VRAM.\n",
    "initial_learning_rate = 1e-4;\n",
    "num_epochs = 50; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_loader = DataLoader(bird_dataset(\"Birds_25\\\\train\"), batch_size=batch_size, shuffle=True); #This is how to use the DataLoader to get batches of data.\n",
    "Test_loader = DataLoader(bird_dataset(\"Birds_25\\\\test\"), batch_size=batch_size, shuffle=True);\n",
    "Val_loader = DataLoader(bird_dataset(\"Birds_25\\\\val\"), batch_size=batch_size, shuffle=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(in_channels, num_classes, n).to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= initial_learning_rate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"\\nepoch: \", epoch+1);\n",
    "        for i, (images, labels) in enumerate(Train_loader):\n",
    "            images = images.to(device);\n",
    "            labels = labels.to(device);\n",
    "\n",
    "            #Forward pass\n",
    "            outputs = model(images);\n",
    "            # print(outputs); \n",
    "            print(outputs.shape);\n",
    "            print(labels.shape);\n",
    "            # print(labels);\n",
    "            loss = criterion(outputs, labels);\n",
    "            #Backward pass\n",
    "            optimizer.zero_grad(); #Zeroes the gradients before backpropagation.\n",
    "            loss.backward(); #Backpropagation.\n",
    "            optimizer.step(); #Updates the weights.\n",
    "            # if (i+1) % 100 == 0:\n",
    "            #     print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(Train_loader)}], Loss: {loss.item():.4f}')\n",
    "            print(\"batch: \", i+1, \"loss: \", loss.item(), end = \"          \\r\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:  1\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.34 GiB is allocated by PyTorch, and 65.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_model(model)\n",
      "Cell \u001b[1;32mIn[59], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device);\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#Forward pass\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images);\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# print(outputs); \u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape);\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[50], line 34\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres16[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m](x)\n\u001b[0;32m     33\u001b[0m     x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual; \u001b[38;5;66;03m#We add the residual conneciton before passing it to the next layer.\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x); \u001b[38;5;66;03m#The output is complete here.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     residual \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclone(x)\u001b[38;5;241m.\u001b[39mto(device); \n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x;\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\functional.py:1473\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1473\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.34 GiB is allocated by PyTorch, and 65.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_model(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check accuracy on training and test to see how good our model is.\n",
    "def check_accuracy(loader, model):\n",
    "    correct = 0; num_samples = 0;\n",
    "    model.eval(); #Sets it into evaluation mode, so no dropout or batchnorm\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device);\n",
    "            y = y.to(device);\n",
    "            x = x.reshape(x.shape[0], -1);\n",
    "            scores = model(x);\n",
    "            _, predictions = scores.max(1);\n",
    "            correct += (predictions == y).sum();\n",
    "            num_samples += predictions.size(0);\n",
    "\n",
    "        print(f\"Got {correct} / {num_samples} with accuracy {float(correct)/float(num_samples)*100:.2f}\");\n",
    "    model.train();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
