{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import pytorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing the ResNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1, device=device)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels, device=device)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, device=device)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels, device=device)\n",
    "        self.stride = stride;\n",
    "        self.conv1x1 = None; self.bn1x1 = None; #Originally.\n",
    "        if(self.stride != 1):\n",
    "            self.conv1x1 = nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = stride, padding = 0, device=device)\n",
    "            self.bn1x1 = nn.BatchNorm2d(out_channels,device=device);\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x;\n",
    "        o = self.conv1(x)\n",
    "        o = self.bn1(o);\n",
    "        o = self.relu(o).to(device); #The first layer for the resnet block.\n",
    "        o = self.conv2(o); o = self.bn2(o); \n",
    "        if(self.stride != 1): #this means we have to perform 1x1 convolutions\n",
    "            residual = self.conv1x1(residual); residual = self.bn1x1(residual); #Applying the 1x1 convolutions to maintain the size.\n",
    "        o += residual; #inplace addition.\n",
    "        o = self.relu(o); #the second layer output completed here.\n",
    "        return o;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, n):\n",
    "        super(ResNet, self).__init__();\n",
    "        self.n = n;\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, stride=1, padding=1, device=device);\n",
    "        self.bn1 = nn.BatchNorm2d(16); #of output size.\n",
    "        self.relu = nn.ReLU();\n",
    "        self.res16 = [];\n",
    "        for i in range(n):\n",
    "            self.res16.append(ResNetBlock(16,16).to(device));\n",
    "        self.res32 = [ResNetBlock(16,32,2).to(device)]; #1 Block which will change the size of the input.\n",
    "        for i in range(n-1):\n",
    "            self.res32.append(ResNetBlock(32,32).to(device));\n",
    "        self.res64 = [ResNetBlock(32,64,2).to(device)];\n",
    "        for i in range(n-1):\n",
    "            self.res64.append(ResNetBlock(64,64).to(device));\n",
    "        self.final_mean_pool = nn.AvgPool2d(kernel_size=64, stride=1);\n",
    "        self.fc = nn.Linear(64, num_classes);\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device); #incase it is not.\n",
    "        o = self.conv1(x).to(device); o = self.bn1(o).to(device); o = self.relu(o).to(device);\n",
    "        for i in range(self.n):\n",
    "            o = self.res16[i](o).to(device);\n",
    "        for i in range(self.n):\n",
    "            o = self.res32[i](o);\n",
    "        for i in range(self.n):\n",
    "            o = self.res64[i](o);\n",
    "        o = self.final_mean_pool(o); \n",
    "        # o = o.view(o.size(0), -1);\n",
    "        o = torch.flatten(o, start_dim=1); #Flattening from after the batch index.\n",
    "        o = self.fc(o); #final layer.\n",
    "        return o;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet2(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, n):\n",
    "        super(ResNet2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, stride=1, padding=1, device=device); # 16 filters, kernel size 3x3. output shape is same as input shape due to padding = 1\n",
    "        self.n = n;\n",
    "        #Now we have the next 3 blocks of ResNet. \n",
    "        # The first n blocks have 2n filters of size 3x3, and 16 filters, with residual connection between each 2 consecutive filters.\n",
    "        self.res16 = [];\n",
    "        for i in range(2*n): #2n channels of 16 filters each. Need to have residual connections between them too.\n",
    "            self.res16.append(nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, device= device)); \n",
    "\n",
    "        self.res16_32_1x1 = nn.Conv2d(16, 32, kernel_size=1, stride=2, padding=0, device=device); #1x1 convolution to increase the number of filters to 32, and halve the feature map size from 256x256 to 128x128.\n",
    "        self.res32 = [nn.Conv2d(16,32, kernel_size=3, stride=2, padding=1, device=device)]; #Halves the feature map size from 256x256 to 128x128, while increasing filters to 32\n",
    "        for i in range(2*n-1):\n",
    "            self.res32.append(nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, device=device));\n",
    "\n",
    "        self.res32_64_1x1 = nn.Conv2d(32, 64, kernel_size=1, stride=2, padding=0, device=device); #1x1 convolution to increase the number of filters to 64, and halve the feature map size from 128x128 to 64x64.\n",
    "        self.res64 = [nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, device=device)]; #Halves the feature map size from 128x128 to 64x64, while increasing filters to 64\n",
    "        for i in range(2*n-1):\n",
    "            self.res64.append(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, device=device));\n",
    "\n",
    "        self.final_mean_pool = nn.AvgPool2d(kernel_size=64, stride=1); #Average pooling to get the mean of the 64x64 feature map\n",
    "        self.fc = nn.Linear(64, num_classes, device=device); #Fully connected layer to output the class scores\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)); #First Convolutional layer.\n",
    "        #Now we have the next 3 blocks of ResNet.\n",
    "        #For the first n blocks. \n",
    "        for i in range(0,2*self.n-1,2): #with a step of 2, so we pass over each residual connection.\n",
    "            x = F.relu(self.res16[i+1](F.relu(self.res16[i](x))) + x); #The output is complete here.\n",
    "        # return x;\n",
    "        #Now we have a residual of shape 16x256x256, we need to pass it through to the next layer of 32x128x128.\n",
    "        res = self.res16_32_1x1(x); #to match the dimensions of the next layer.\n",
    "        x = F.relu(self.res32[1](F.relu(self.res32[0](x))) + res);\n",
    "        for i in range(2,2*self.n-1,2):\n",
    "            x = F.relu(self.res32[i+1](F.relu(self.res32[i](x))) + x);\n",
    "\n",
    "        res = self.res32_64_1x1(x);\n",
    "        x = F.relu(self.res64[1](F.relu(self.res64[0](x))) + res);\n",
    "        for i in range(2,2*self.n-1,2):\n",
    "            x = F.relu(self.res64[i+1](F.relu(self.res64[i](x))) + x);\n",
    "        # print(\"out_shape: \", x.shape); #The output shape is 64x64x64\n",
    "        x = self.final_mean_pool(x); #Average pooling to get the mean of the 64x64 feature map\n",
    "        # print(\"after avgpool:\" ,x.shape);\n",
    "        x = torch.flatten(x, start_dim=1); #Flatten the output to pass it to the fully connected layer. The first dimension is the batch.\n",
    "        x = self.fc(x); #Fully connected layer to output the class scores\n",
    "        return x;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bird_dataset(Dataset):\n",
    "    def __init__(self, datapath): #Either test, train, or val datafolder.\n",
    "        self.datapath = datapath;\n",
    "        folder_list = glob.glob(datapath + \"/*\");\n",
    "        self.data = [];\n",
    "        self.labels = set();\n",
    "        for folder in folder_list:\n",
    "            label = os.path.basename(folder); #gets the last name of the folder, which is the label.\n",
    "            self.labels.add(label);\n",
    "            file_list = glob.glob(folder + \"/*\");\n",
    "            for file in file_list:\n",
    "                self.data.append((file, label));\n",
    "        self.labels = list(self.labels);\n",
    "        self.label_to_index = {label: i for i, label in enumerate(self.labels)};\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data);\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx];\n",
    "        img = read_image(img_path)\n",
    "        img = img/255;\n",
    "        # print(img);\n",
    "        # img = transforms.ToTensor()(img); #converts the image to a tensor, but read_image already does this.\n",
    "        label = self.label_to_index[label]; #using labels as indices for the classes, instead of names.\n",
    "        # label_arr = np.zeros(len(self.labels));\n",
    "        # label_arr[label] = 1;\n",
    "        return img, label;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters for the network.32\n",
    "num_classes = 25; \n",
    "n = 2; #6n + 2 layers.\n",
    "in_channels = 3; #RGB images.\n",
    "batch_size = 32; #Probably wont run on my laptop with just 4GB of VRAM.\n",
    "initial_learning_rate = 0.01;\n",
    "num_epochs = 50; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_loader = DataLoader(bird_dataset(\"Birds_25\\\\train\"), batch_size=batch_size, shuffle=True); #This is how to use the DataLoader to get batches of data.\n",
    "Test_loader = DataLoader(bird_dataset(\"Birds_25\\\\test\"), batch_size=batch_size, shuffle=True);\n",
    "Val_loader = DataLoader(bird_dataset(\"Birds_25\\\\val\"), batch_size=batch_size, shuffle=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(in_channels, num_classes, n).to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr= initial_learning_rate);\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_learning_rate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filename):\n",
    "    checkpoint = torch.load(filename);\n",
    "    model.load_state_dict(checkpoint['model_state_dict']);\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict']);\n",
    "    epoch = checkpoint['epoch'];\n",
    "    loss = checkpoint['loss'];\n",
    "    return model, optimizer, epoch, loss;\n",
    "\n",
    "def store_checkpoint(model, optimizer, epoch, loss, filename):\n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': loss,\n",
    "            }, filename);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model):\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time();\n",
    "        mean_loss = 0; total_batches = 0;\n",
    "        print(\"epoch: \", epoch+1);\n",
    "        for i, (images, labels) in enumerate(Train_loader):\n",
    "            total_batches += 1;\n",
    "            images = images.to(device);\n",
    "            labels = labels.to(device);\n",
    "            #Forward pass\n",
    "            outputs = model(images);\n",
    "            loss = criterion(outputs, labels);\n",
    "            #Backward pass\n",
    "            optimizer.zero_grad(); #Zeroes the gradients before backpropagation.\n",
    "            loss.backward(); #Backpropagation.\n",
    "            optimizer.step(); #Updates the weights.\n",
    "            mean_loss += loss.item();\n",
    "            if(i%25 == 0):\n",
    "                store_checkpoint(model, optimizer, epoch, loss.item(), \"checkpoint.pth\");\n",
    "            print(\"batch: \", i+1, \"loss: \", loss.item(), end = \"          \\r\");\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = g['lr'] - 0.0002; #Decay the learning rate by a constant after each epoch.\n",
    "        end = time.time();\n",
    "        store_checkpoint(model, optimizer, epoch, loss.item(), \"checkpoint\" + str(epoch) + \".pth\");\n",
    "        print(epoch, \"th epoch: \", end-start, \" seconds,   \", \"mean loss: \", mean_loss/total_batches, \"          \");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[[[ 0.1041, -0.0521,  0.0380],\n",
       "             [-0.1021, -0.1249, -0.0302],\n",
       "             [ 0.1809, -0.0348, -0.1300]],\n",
       "   \n",
       "            [[ 0.0776,  0.0891,  0.0892],\n",
       "             [ 0.1496,  0.0456, -0.0308],\n",
       "             [ 0.0790, -0.1611,  0.1430]],\n",
       "   \n",
       "            [[-0.0037,  0.1883,  0.0823],\n",
       "             [ 0.1555,  0.0182, -0.1179],\n",
       "             [-0.1915,  0.1807,  0.0921]]],\n",
       "   \n",
       "   \n",
       "           [[[-0.1933,  0.1258, -0.1131],\n",
       "             [-0.1316,  0.1882, -0.0891],\n",
       "             [ 0.1894,  0.0360, -0.1410]],\n",
       "   \n",
       "            [[-0.0986,  0.0218,  0.0746],\n",
       "             [ 0.0757,  0.0659,  0.1453],\n",
       "             [ 0.1413,  0.1021,  0.0052]],\n",
       "   \n",
       "            [[-0.0604,  0.1582,  0.0961],\n",
       "             [-0.1487, -0.0441,  0.0267],\n",
       "             [-0.0302, -0.0811, -0.1219]]],\n",
       "   \n",
       "   \n",
       "           [[[ 0.1212, -0.1124, -0.1291],\n",
       "             [ 0.1064, -0.0710,  0.0346],\n",
       "             [ 0.0372,  0.1383, -0.0272]],\n",
       "   \n",
       "            [[ 0.1873, -0.0109, -0.0905],\n",
       "             [-0.0748,  0.0962,  0.1860],\n",
       "             [ 0.0328,  0.1189,  0.1327]],\n",
       "   \n",
       "            [[-0.0076,  0.0625,  0.0521],\n",
       "             [-0.1898,  0.1290, -0.0502],\n",
       "             [-0.1551,  0.1411, -0.1472]]],\n",
       "   \n",
       "   \n",
       "           [[[-0.0773, -0.1400, -0.1365],\n",
       "             [-0.1437,  0.0839,  0.1913],\n",
       "             [ 0.0608, -0.0414,  0.0713]],\n",
       "   \n",
       "            [[ 0.0759, -0.0159, -0.0756],\n",
       "             [ 0.1054,  0.0732, -0.0856],\n",
       "             [-0.0769, -0.1522,  0.0271]],\n",
       "   \n",
       "            [[ 0.1691, -0.1196, -0.0756],\n",
       "             [-0.0645,  0.0719, -0.0173],\n",
       "             [-0.0576,  0.1137, -0.0455]]],\n",
       "   \n",
       "   \n",
       "           [[[ 0.0652,  0.0838, -0.1280],\n",
       "             [-0.0040,  0.0005,  0.0602],\n",
       "             [ 0.1638, -0.1750,  0.1072]],\n",
       "   \n",
       "            [[-0.1533,  0.1586, -0.0781],\n",
       "             [-0.1734, -0.0315, -0.0354],\n",
       "             [-0.0729,  0.1409, -0.1348]],\n",
       "   \n",
       "            [[-0.0849,  0.0999,  0.1124],\n",
       "             [ 0.0977,  0.1583, -0.1483],\n",
       "             [ 0.0481,  0.0285,  0.0192]]],\n",
       "   \n",
       "   \n",
       "           [[[-0.1602,  0.1725, -0.1571],\n",
       "             [-0.0184,  0.0995,  0.0748],\n",
       "             [-0.1723, -0.1440,  0.1405]],\n",
       "   \n",
       "            [[-0.0026,  0.1233,  0.1738],\n",
       "             [-0.1813, -0.0180,  0.0948],\n",
       "             [-0.0989,  0.0738, -0.0605]],\n",
       "   \n",
       "            [[-0.0901, -0.0363, -0.0625],\n",
       "             [-0.0595,  0.1295,  0.0681],\n",
       "             [ 0.0795, -0.1136,  0.0396]]],\n",
       "   \n",
       "   \n",
       "           [[[-0.1527, -0.0461,  0.0617],\n",
       "             [ 0.1064,  0.1014, -0.0749],\n",
       "             [-0.0033,  0.0415, -0.1102]],\n",
       "   \n",
       "            [[-0.0393, -0.1489,  0.1733],\n",
       "             [-0.1764,  0.0019, -0.0680],\n",
       "             [-0.0265, -0.1536,  0.1001]],\n",
       "   \n",
       "            [[ 0.1340, -0.1283,  0.0400],\n",
       "             [ 0.0382, -0.1711, -0.0221],\n",
       "             [-0.1344, -0.0646, -0.0820]]],\n",
       "   \n",
       "   \n",
       "           [[[ 0.0509,  0.1619, -0.0123],\n",
       "             [ 0.1289,  0.0533,  0.0048],\n",
       "             [ 0.1766,  0.1343, -0.1029]],\n",
       "   \n",
       "            [[ 0.1433,  0.0257,  0.1194],\n",
       "             [ 0.1679,  0.1121, -0.1265],\n",
       "             [ 0.0161, -0.0447,  0.0062]],\n",
       "   \n",
       "            [[ 0.1213, -0.0744, -0.1240],\n",
       "             [-0.0905,  0.0318, -0.1543],\n",
       "             [ 0.0482,  0.1769, -0.1720]]],\n",
       "   \n",
       "   \n",
       "           [[[ 0.0126, -0.0605, -0.0598],\n",
       "             [ 0.0582,  0.0463, -0.0006],\n",
       "             [-0.1340,  0.1053,  0.1059]],\n",
       "   \n",
       "            [[ 0.1274, -0.0228, -0.1604],\n",
       "             [-0.0523,  0.1041,  0.1776],\n",
       "             [ 0.1623,  0.1615, -0.0062]],\n",
       "   \n",
       "            [[-0.1107,  0.0400,  0.0540],\n",
       "             [ 0.1088, -0.0982, -0.1073],\n",
       "             [ 0.0882, -0.0092, -0.0409]]],\n",
       "   \n",
       "   \n",
       "           [[[-0.1467,  0.1350, -0.0639],\n",
       "             [ 0.0500,  0.0078,  0.0676],\n",
       "             [-0.1721, -0.0214,  0.0391]],\n",
       "   \n",
       "            [[ 0.0522,  0.1870, -0.0490],\n",
       "             [-0.1628, -0.0119,  0.0645],\n",
       "             [-0.0314, -0.0209,  0.1364]],\n",
       "   \n",
       "            [[-0.1876,  0.0733,  0.0094],\n",
       "             [-0.1654,  0.1572, -0.0292],\n",
       "             [ 0.0995, -0.0024,  0.1812]]],\n",
       "   \n",
       "   \n",
       "           [[[-0.1867,  0.0091, -0.0395],\n",
       "             [-0.0217,  0.1911, -0.0106],\n",
       "             [-0.0015, -0.1125,  0.0507]],\n",
       "   \n",
       "            [[-0.1747,  0.1448,  0.0974],\n",
       "             [ 0.1349,  0.1741,  0.1594],\n",
       "             [-0.1760, -0.0394, -0.0009]],\n",
       "   \n",
       "            [[ 0.1585,  0.1607,  0.1725],\n",
       "             [ 0.0446, -0.1497,  0.1880],\n",
       "             [-0.0822,  0.0498, -0.1767]]],\n",
       "   \n",
       "   \n",
       "           [[[ 0.1708, -0.1722,  0.1230],\n",
       "             [-0.1111, -0.0948,  0.1143],\n",
       "             [ 0.1713, -0.0438,  0.0381]],\n",
       "   \n",
       "            [[ 0.1904,  0.0240, -0.1531],\n",
       "             [ 0.1691, -0.1206, -0.0171],\n",
       "             [ 0.0204,  0.1173,  0.0408]],\n",
       "   \n",
       "            [[ 0.1623, -0.0966,  0.1931],\n",
       "             [-0.1349, -0.0693, -0.0417],\n",
       "             [-0.1190, -0.0172, -0.0250]]],\n",
       "   \n",
       "   \n",
       "           [[[ 0.0336, -0.0267, -0.1128],\n",
       "             [ 0.1070, -0.0402, -0.1478],\n",
       "             [ 0.0510, -0.1910,  0.0066]],\n",
       "   \n",
       "            [[ 0.1373,  0.1621,  0.1093],\n",
       "             [-0.1320,  0.0253, -0.1442],\n",
       "             [-0.1915, -0.1019,  0.1876]],\n",
       "   \n",
       "            [[ 0.1307,  0.0817, -0.0506],\n",
       "             [-0.1259, -0.0596, -0.1155],\n",
       "             [ 0.0577,  0.0095, -0.0356]]],\n",
       "   \n",
       "   \n",
       "           [[[ 0.0431, -0.0538,  0.1327],\n",
       "             [-0.1114,  0.0113,  0.1704],\n",
       "             [ 0.0921,  0.0253, -0.0675]],\n",
       "   \n",
       "            [[-0.1785,  0.1878,  0.1669],\n",
       "             [-0.0422, -0.0984,  0.1337],\n",
       "             [ 0.1471, -0.0686, -0.1030]],\n",
       "   \n",
       "            [[ 0.0903, -0.0999, -0.1147],\n",
       "             [-0.1778, -0.0718, -0.1481],\n",
       "             [ 0.0703, -0.0442, -0.1882]]],\n",
       "   \n",
       "   \n",
       "           [[[ 0.0097, -0.0967,  0.0835],\n",
       "             [-0.0571,  0.0643,  0.1741],\n",
       "             [ 0.0217, -0.1654,  0.1288]],\n",
       "   \n",
       "            [[ 0.1299,  0.0642,  0.1373],\n",
       "             [-0.0864, -0.1198,  0.0379],\n",
       "             [ 0.0206, -0.0948,  0.0815]],\n",
       "   \n",
       "            [[-0.1624, -0.1118, -0.1734],\n",
       "             [-0.0193,  0.0815,  0.0836],\n",
       "             [-0.1608, -0.1428, -0.0153]]],\n",
       "   \n",
       "   \n",
       "           [[[-0.1064,  0.0161, -0.1299],\n",
       "             [ 0.1001,  0.1015,  0.0739],\n",
       "             [-0.0893, -0.0994, -0.1317]],\n",
       "   \n",
       "            [[-0.0769,  0.0800, -0.0912],\n",
       "             [-0.0546, -0.1752, -0.1273],\n",
       "             [ 0.1628,  0.1413, -0.0418]],\n",
       "   \n",
       "            [[-0.1912,  0.1351,  0.1299],\n",
       "             [-0.1810, -0.1620, -0.0141],\n",
       "             [-0.1213,  0.1759, -0.1850]]]], device='cuda:0', requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-0.1438, -0.1918, -0.0898, -0.0746,  0.1520,  0.1168,  0.1770, -0.0552,\n",
       "           -0.1520,  0.0737, -0.0764,  0.0014,  0.0144,  0.0966,  0.0437, -0.0940],\n",
       "          device='cuda:0', requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([1.0013, 1.0001, 0.9995, 1.0006, 0.9990, 1.0003, 1.0006, 1.0006, 0.9982,\n",
       "           1.0002, 1.0004, 0.9999, 1.0008, 0.9996, 1.0001, 0.9997],\n",
       "          device='cuda:0', requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([ 4.7847e-04, -1.7430e-04,  1.9516e-04,  2.6747e-05, -3.4015e-04,\n",
       "            2.6612e-04,  1.3453e-04,  2.6981e-04, -3.6328e-04, -3.5241e-04,\n",
       "            5.0641e-04, -1.8758e-04,  4.5462e-04,  8.8251e-05,  1.9929e-04,\n",
       "           -2.0745e-04], device='cuda:0', requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[ 0.0576, -0.0575,  0.0157,  ...,  0.1017, -0.0673,  0.0026],\n",
       "           [-0.0456, -0.1194, -0.0017,  ..., -0.0831, -0.0605,  0.0795],\n",
       "           [ 0.0079, -0.1091, -0.0079,  ..., -0.0258, -0.0149, -0.1048],\n",
       "           ...,\n",
       "           [ 0.0748,  0.0805,  0.0615,  ..., -0.1001,  0.0325,  0.0239],\n",
       "           [ 0.1050, -0.0961,  0.1026,  ..., -0.0475, -0.0589, -0.0570],\n",
       "           [ 0.0670, -0.0902, -0.0814,  ..., -0.0279,  0.0471,  0.0685]],\n",
       "          device='cuda:0', requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([ 0.0765,  0.0269, -0.0085, -0.1100,  0.0729,  0.1177, -0.0008, -0.0602,\n",
       "            0.0288, -0.1183,  0.0316, -0.1133,  0.0316,  0.0754, -0.1224, -0.0605,\n",
       "           -0.0476,  0.1023, -0.0200,  0.0454, -0.0221,  0.0258,  0.0446, -0.1162,\n",
       "            0.0093], device='cuda:0', requires_grad=True)],\n",
       "  'lr': 0.01,\n",
       "  'momentum': 0,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0,\n",
       "  'nesterov': False,\n",
       "  'maximize': False,\n",
       "  'foreach': None,\n",
       "  'differentiable': False}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_checkpoint(model, optimizer, \"checkpoint.pth\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  4 loss:  3.365556001663208          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_model(model)\n",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward(); \u001b[38;5;66;03m#Backpropagation.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep(); \u001b[38;5;66;03m#Updates the weights.\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m mean_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem();\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m25\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     19\u001b[0m     store_checkpoint(model, optimizer, epoch, loss\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m);\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check accuracy on training and test to see how good our model is.\n",
    "def check_accuracy(loader, model):\n",
    "    correct = 0; num_samples = 0;\n",
    "    model.eval(); #Sets it into evaluation mode, so no dropout or batchnorm\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device);\n",
    "            y = y.to(device);\n",
    "            #x = x.reshape(x.shape[0], -1);\n",
    "            scores = model(x);\n",
    "            _, predictions = scores.max(1);\n",
    "            correct += (predictions == y).sum();\n",
    "            num_samples += predictions.size(0);\n",
    "            print(\"partial: \", correct, \"/\", num_samples, \" = \", correct/num_samples, \"%\", end = \"          \\r\");\n",
    "\n",
    "        print(f\"Got {correct} / {num_samples} with accuracy {float(correct)/float(num_samples)*100:.2f}\");\n",
    "    model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial:  tensor(166, device='cuda:0') / 3296  =  tensor(0.0504, device='cuda:0') %          or(3, device='cuda:0') / 64  =  tensor(0.0469, device='cuda:0') %          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load_checkpoint(model, optimizer, \"checkpoint.pth\");\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m check_accuracy(Val_loader, model);\n\u001b[0;32m      3\u001b[0m check_accuracy(Test_loader, model)\n",
      "Cell \u001b[1;32mIn[48], line 6\u001b[0m, in \u001b[0;36mcheck_accuracy\u001b[1;34m(loader, model)\u001b[0m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval(); \u001b[38;5;66;03m#Sets it into evaluation mode, so no dropout or batchnorm\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x,y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m      7\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device);\n\u001b[0;32m      8\u001b[0m         y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device);\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[36], line 21\u001b[0m, in \u001b[0;36mbird_dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     20\u001b[0m     img_path, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx];\n\u001b[1;32m---> 21\u001b[0m     img \u001b[38;5;241m=\u001b[39m read_image(img_path)\n\u001b[0;32m     22\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m;\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# print(img);\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# img = transforms.ToTensor()(img); #converts the image to a tensor, but read_image already does this.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torchvision\\io\\image.py:258\u001b[0m, in \u001b[0;36mread_image\u001b[1;34m(path, mode)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m    257\u001b[0m     _log_api_usage_once(read_image)\n\u001b[1;32m--> 258\u001b[0m data \u001b[38;5;241m=\u001b[39m read_file(path)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torchvision\\io\\image.py:52\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m     51\u001b[0m     _log_api_usage_once(read_file)\n\u001b[1;32m---> 52\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mread_file(path)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\_ops.py:755\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load_checkpoint(model, optimizer, \"checkpoint.pth\");\n",
    "check_accuracy(Val_loader, model);\n",
    "check_accuracy(Test_loader, model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2835 / 8413 with accuracy 33.70\n",
      "Got 2518 / 7500 with accuracy 33.57\n"
     ]
    }
   ],
   "source": [
    "# load_checkpoint(model, optimizer, \"checkpoint_kaggle.pth\");\n",
    "check_accuracy(Val_loader, model);\n",
    "check_accuracy(Test_loader, model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_checkpoint(model, optimizer, 51, 2, \"chkpt.pth\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
