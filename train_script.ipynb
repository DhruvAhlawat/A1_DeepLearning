{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","from torchvision.io import read_image\n","import glob\n","import time\n","import os\n","import torchmetrics\n","import matplotlib.pyplot as plt\n","import pickle\n","from torchmetrics.classification import MulticlassF1Score, MulticlassAccuracy\n","from torchvision.datasets import ImageFolder\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\");"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# part A : Batch Normalisation\n","class BatchNorm2d(nn.Module): #My definition of Batch Normalisation.\n","    def __init__(self, size):\n","        super(BatchNorm2d, self).__init__()\n","        self.epsilon = 1e-5;\n","        shape = (1, size, 1, 1)\n","        self.gamma = nn.Parameter(torch.ones(shape)) # the scaling factor that determines the new standard deviation.\n","        self.beta = nn.Parameter(torch.zeros(shape)) # the bias that is the new mean.\n","        self.running_sum = torch.zeros(shape).to(device);\n","        self.running_square_sum = torch.zeros(shape).to(device);\n","        self.total = 0;\n","    def forward(self, X):\n","        #if we are in training mode, then we use the mean and variance of this batch.\n","        if self.training:\n","            mean = torch.mean(X, dim = (0,2,3), keepdim = True);\n","            var = torch.var(X,dim = (0,2,3), keepdim = True);\n","            self.total += 1;\n","            self.running_sum += mean;\n","            self.running_square_sum += var;\n","        else:\n","            mean = self.running_sum / self.total;\n","            var = self.running_square_sum / self.total;\n","        # X_mean = torch.ones(X.shape) * mean;\n","#         X_mean = mean.expand_as(X);\n","        X_transformed = (X - mean) / torch.sqrt(var + self.epsilon); #epsilon is added for non-zero denominator\n","        # X_transformed = self.gamma * X_transformed + self.beta;\n","        X_transformed = X_transformed * self.gamma + self.beta;\n","        return X_transformed;\n","class InstanceNormalisation2d(nn.Module):\n","    def __init__(self, size):\n","        super(InstanceNormalisation2d, self).__init__();\n","        self.epsilon = 1e-5;\n","        self.gamma = nn.Parameter(torch.ones((1, size, 1, 1)));\n","        self.beta = nn.Parameter(torch.zeros((1, size, 1, 1)));\n","    def forward(self, X):\n","        mean = torch.mean(X, dim = (2,3), keepdim = True);\n","        var = torch.var(X, dim = (2,3), keepdim = True);\n","        X_transformed = (X - mean) / torch.sqrt(var + self.epsilon);\n","        X_transformed = X_transformed * self.gamma + self.beta;\n","        return X_transformed;\n","class BatchInstanceNormalisation2d(nn.Module):\n","    def __init__(self, size):\n","        super(BatchInstanceNormalisation2d, self).__init__();\n","        self.batch_norm = BatchNorm2d(size);\n","        self.instance_norm = InstanceNormalisation2d(size);\n","        shape = (1, size, 1, 1)\n","        self.rho = nn.Parameter(torch.ones(shape));\n","        self.epsilon = 1e-5;\n","        self.gamma = nn.Parameter(torch.ones(shape)) # the scaling factor that determines the new standard deviation.\n","        self.beta = nn.Parameter(torch.zeros(shape)) # the bias that is the new mean.\n","    def forward(self, X):\n","        #if we are in training mode, then we use the mean and variance of this batch.\n","        X_batch = self.batch_norm(X);\n","        X_instance = self.instance_norm(X);\n","        #X_batch = (X - mean) / torch.sqrt(var + self.epsilon); #epsilon is added for non-zero denominator\n","        #instance_mean = torch.mean(X, dim = (2,3), keepdim = True);\n","        #instance_var = torch.var(X, dim = (2,3), keepdim = True);\n","        #X_instance = (X - instance_mean) / torch.sqrt(instance_var + self.epsilon); #this is the instance value.\n","        X_transformed = self.rho * X_batch + (1 - self.rho) * X_instance;\n","        X_transformed = X_transformed * self.gamma + self.beta;\n","        return X_transformed;\n","class LayerNormalisation2d(nn.Module):\n","    def __init__(self, size = None):\n","        super(LayerNormalisation2d, self).__init__();\n","        self.epsilon = 1e-5; #it actually has no use for size, since it normalizes accross the channels as well.\n","        self.gamma = nn.Parameter(torch.ones((1, 1, 1, 1)));\n","        self.beta = nn.Parameter(torch.zeros((1, 1, 1, 1)));\n","    def forward(self, X):\n","        mean = torch.mean(X, dim = (1,2,3), keepdim = True); #normalizes accross the channel dimension as well.\n","        var = torch.var(X, dim = (1,2,3), keepdim = True);\n","        X_transformed = (X - mean) / torch.sqrt(var + self.epsilon);\n","        X_transformed = X_transformed * self.gamma + self.beta;\n","        return X_transformed;\n","class GroupNormalisation2d(nn.Module):\n","    def __init__(self, size, groups = 8):\n","        super(GroupNormalisation2d, self).__init__();\n","        self.epsilon = 1e-5;\n","        self.gamma = nn.Parameter(torch.ones((1, size, 1, 1)));\n","        self.beta = nn.Parameter(torch.zeros((1, size, 1, 1)));\n","        self.groups = groups;\n","    def forward(self, X):\n","        shape = X.shape;\n","        X = X.view(shape[0], self.groups, shape[1]//self.groups, shape[2], shape[3]);\n","        mean = torch.mean(X, dim = (2,3,4), keepdim = True);\n","        var = torch.var(X, dim = (2,3,4), keepdim = True);\n","        X_transformed = (X - mean) / torch.sqrt(var + self.epsilon);\n","        X_transformed = X_transformed.view(shape);\n","        X_transformed = X_transformed * self.gamma + self.beta;\n","        return X_transformed;\n","class NoNormalisation(nn.Module):\n","    def __init__(self, size):\n","        super(NoNormalisation, self).__init__();\n","    def forward(self, X):\n","        return X; #no transformations applied."]},{"cell_type":"markdown","metadata":{},"source":["### Describing the ResNet class"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ResNetBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride = 1, norm = nn.BatchNorm2d):\n","        super(ResNetBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1,bias=False)\n","        self.bn1 = norm(out_channels).to(device=device)\n","        self.relu = nn.ReLU()\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1,bias=False)\n","        self.bn2 = norm(out_channels).to(device=device)\n","        self.stride = stride;\n","        self.conv1x1 = None; self.bn1x1 = None; #Originally.\n","        if(self.stride != 1):\n","            self.conv1x1 = nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = stride, padding = 0,bias=False)\n","            self.bn1x1 = nn.BatchNorm2d(out_channels,device=device);\n","\n","    def forward(self, x):\n","#         residual = x;\n","        o = self.conv1(x)\n","        o = self.bn1(o);\n","        o = F.relu(o).to(device); #The first layer for the resnet block.\n","        o = self.conv2(o); \n","        o = self.bn2(o); \n","        if(self.stride != 1): #this means we have to perform 1x1 convolutions\n","            x = self.conv1x1(x); \n","            x = self.bn1x1(x); #Applying the 1x1 convolutions to maintain the size.\n","        o += x; #inplace addition.\n","        o = F.relu(o); #the second layer output completed here.\n","        return o;\n","class ResNet(nn.Module):\n","    def __init__(self, in_channels, num_classes, n, norm=nn.BatchNorm2d):\n","        super(ResNet, self).__init__();\n","        self.n = n;\n","        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, stride=1, padding=1, device=device,bias=False);\n","        self.bn1 = norm(16).to(device=device); #of output size.\n","        self.relu = nn.ReLU();\n","        self.res16 = nn.ModuleList();\n","        for i in range(n):\n","            self.res16.append(ResNetBlock(16,16, norm=norm).to(device));\n","        self.res32 = nn.ModuleList();\n","        self.res32.append(ResNetBlock(16,32,2, norm=norm).to(device)); #1 Block which will change the size of the input.\n","        for i in range(n-1):\n","            self.res32.append(ResNetBlock(32,32,norm=norm).to(device));\n","        self.res64 = nn.ModuleList();\n","        self.res64.append(ResNetBlock(32,64,2,norm=norm).to(device));\n","        for i in range(n-1):\n","            self.res64.append(ResNetBlock(64,64,norm=norm).to(device));\n","        \n","        self.final_mean_pool = nn.AdaptiveAvgPool2d(output_size=(1,1));\n","        self.fc = nn.Linear(64, num_classes);\n","\n","    def forward(self, o):\n","        o = self.conv1(o)\n","        o = self.bn1(o)\n","        o = self.relu(o)\n","        for i in range(len(self.res16)):\n","            o = self.res16[i](o)\n","        for i in range(len(self.res32)):\n","            o = self.res32[i](o);\n","        for i in range(len(self.res64)):\n","            o = self.res64[i](o);\n","        o = self.final_mean_pool(o); \n","        o = o.view(o.size(0), -1);\n","#         o = torch.flatten(o, start_dim=1); #Flattening from after the batch index.\n","        o = self.fc(o); #final layer.\n","        return o;"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["class bird_dataset(Dataset):\n","    def __init__(self, datapath): #Either test, train, or val datafolder.\n","        self.datapath = datapath;\n","        folder_list = glob.glob(datapath + \"/*\");\n","        self.data = [];\n","        self.labels = set();\n","        for folder in folder_list:\n","            label = os.path.basename(folder); #gets the last name of the folder, which is the label.\n","            self.labels.add(label);\n","            file_list = glob.glob(folder + \"/*\");\n","            for file in file_list:\n","                self.data.append((file, label));\n","        self.labels = list(self.labels);\n","        self.label_to_index = {label: i for i, label in enumerate(self.labels)};\n","    \n","    def __len__(self):\n","        return len(self.data);\n","    \n","    def __getitem__(self, idx):\n","        img_path, label = self.data[idx];\n","        img = read_image(img_path)\n","        img = img/255;\n","        # print(img);\n","        # img = transforms.ToTensor()(img); #converts the image to a tensor, but read_image already does this.\n","        label = self.label_to_index[label]; #using labels as indices for the classes, instead of names.\n","        # label_arr = np.zeros(len(self.labels));\n","        # label_arr[label] = 1;\n","        return img, label;"]},{"cell_type":"markdown","metadata":{},"source":["### creating the dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["## Parameters for the network.32\n","num_classes = 25; \n","n = 2; #6n + 2 layers.\n","in_channels = 3; #RGB images.\n","batch_size = 32; #Probably wont run on my laptop with just 4GB of VRAM.\n","initial_learning_rate = 0.003;\n","num_epochs = 50; "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset = ImageFolder(root=\"/kaggle/input/birds-2/Birds_25/train\",transform=transforms.ToTensor())\n","test_dataset = ImageFolder(root=\"/kaggle/input/birds-2/Birds_25/test\",transform=transforms.ToTensor())\n","val_dataset = ImageFolder(root=\"/kaggle/input/birds-2/Birds_25/val\",transform=transforms.ToTensor())\n","# Train_loader = DataLoader(bird_dataset(\"Birds_25/train\"), batch_size=batch_size, shuffle=True); #This is how to use the DataLoader to get batches of data.\n","Train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers = 4);\n","Test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers = 2);\n","Val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers = 2);\n","\n","# Test_loader = DataLoader(bird_dataset(\"/kaggle/input/birds-2/Birds_25/test\"), batch_size=batch_size, shuffle=True);\n","# Val_loader = DataLoader(bird_dataset(\"/kaggle/input/birds-2/Birds_25/val\"), batch_size=batch_size, shuffle=True);"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# model = ResNet(in_channels, num_classes, n, norm=BatchNorm2d).to(device);\n","\n","#model = ResNet(in_channels, num_classes, n).to(device);\n","#print(\"Doing Inbuilt implementation with batch size 32\");\n","model = ResNet(in_channels, num_classes, n, norm=GroupNormalisation2d).to(device);\n","print( \"DOING MY IMPLEMENTATION OF Group Normalisation with batch_size\", batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr= initial_learning_rate);\n","# optimizer = optim.SGD(model.parameters(), lr = initial_learning_rate, momentum = 0.92);"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def load_checkpoint(model, optimizer, filename):\n","    checkpoint = torch.load(filename);\n","    model.load_state_dict(checkpoint['model_state_dict']);\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict']);\n","    epoch = checkpoint['epoch'];\n","    loss = checkpoint['loss'];\n","    return model, optimizer, epoch, loss;\n","\n","def store_checkpoint(model, optimizer, epoch, loss, filename):\n","    torch.save({\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'epoch': epoch,\n","            'loss': loss,\n","            }, filename);"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Check accuracy on training and test to see how good our model is.\n","macroF1 = MulticlassF1Score(num_classes=num_classes, average='macro')\n","microF1 = MulticlassF1Score(num_classes=num_classes, average='micro')\n","accuracy = MulticlassAccuracy(num_classes=num_classes)\n","def check_eval(loader, model):\n","    correct = 0; num_samples = 0;\n","    model.eval(); #Sets it into evaluation mode, so no dropout or batchnorm\n","    preds = []; labels = [];\n","    with torch.no_grad():\n","        for x,y in loader:\n","            x = x.to(device);\n","            y = y.to(device);\n","            #x = x.reshape(x.shape[0], -1);\n","            scores = model(x);\n","            _, predictions = scores.max(1);\n","            preds.extend(predictions);\n","            labels.extend(y);\n","    \n","    preds = torch.tensor(preds); labels = torch.tensor(labels);\n","    acc = accuracy(preds, labels); macF1 = macroF1(preds, labels); micF1 = microF1(preds, labels);\n","    model.train();\n","    return (acc, macF1, micF1);"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# print(check_eval(Train_loader, model));\n","# print(check_eval(Val_loader, model));\n","# print(check_eval(Test_loader, model));"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","def train_model(model, traindata):\n","    for epoch in range(num_epochs):\n","        start = time.time();\n","        mean_loss = 0; total_batches = 0;\n","        print(\"epoch: \", epoch+1);\n","        epochlabels = []; epochoutputs = [];\n","        for i, (images, labels) in enumerate(Train_loader):\n","            total_batches += 1;\n","            images = images.to(device);\n","            labels = labels.to(device);\n","            #Forward pass\n","            outputs = model(images);\n","            loss = criterion(outputs, labels);\n","            #Backward pass\n","            optimizer.zero_grad(); #Zeroes the gradients before backpropagation.\n","            loss.backward(); #Backpropagation.\n","            optimizer.step(); #Updates the weights.\n","            mean_loss += loss.item();\n","            _, preds = outputs.max(1);\n","            epochlabels.extend(labels);\n","            epochoutputs.extend(preds); #stores the values predicted each epoch.\n","            print(\"batch: \", i+1, \"loss: \", mean_loss/total_batches, end = \"          \\r\");\n","            break;\n","        epoch_time = time.time() - start;\n","        for g in optimizer.param_groups:\n","            g['lr'] = g['lr']/1.035; #Decay the learning rate by a constant after each epoch.\n","        store_checkpoint(model, optimizer, epoch, loss.item(), \"checkpoint_epoch\" + str(epoch) + \".pth\");\n","        epochlabels = torch.tensor(epochlabels);\n","        epochoutputs = torch.tensor(epochoutputs);\n","        mic = microF1(epochoutputs, epochlabels); mac = macroF1(epochoutputs, epochlabels); acc = accuracy(epochoutputs, epochlabels);\n","        traindata['train'].append((acc, mac, mic));\n","        valacc, valmac, valmic = check_eval(Val_loader, model);\n","        traindata['val'].append((valacc, valmac, valmic));\n","        testacc, testmac, testmic = check_eval(Test_loader, model);\n","        traindata['test'].append((testacc, testmac, testmic));\n","        traindata['loss'].append(mean_loss/total_batches);\n","        end = time.time();\n","        #print(valacc.item());\n","        #print(traindata['val'][-1][0].item());\n","        print(epoch+1, \"th epoch: \", epoch_time, \"s, total:\", end - start ,\"mean loss: \", mean_loss/total_batches, \"          \");\n","        print(\"VAL acc, mac, mic :\", (valacc, valmac, valmic))\n","        #for name, param in model.named_parameters():\n","        #    if name.split('.')[-1] == 'rho':\n","        #       setattr(model, name, torch.clamp(param, 0, 1)) #clamping it for Batch Instance normalization\n","        #if(epoch > 8):\n","        #    if(valacc.item() < traindata['val'][-2][0].item() and valacc.item() < traindata['val'][-3][0].item()):\n","        #        print(\"early stopping due to decreasing Accuracy here\");\n","                "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["load_checkpoint(model, optimizer, '/kaggle/input/groupnorm/part_1.2_gn.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["traindata = {};\n","traindata['val'] = [];\n","traindata['train'] = [];\n","traindata['test'] = [];\n","traindata['loss'] = [];\n","train_start = time.time();\n","train_model(model, traindata);\n","duration = time.time() - train_start;\n","print(\"\\n\\n---------Training finished after \", duration, \" seconds--------\\n\\n\");"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n","from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n","from pytorch_grad_cam.utils.image import show_cam_on_image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["target_layers = [model.fc]\n","for i, (images, labels) in enumerate(Train_loader):\n","    input_tensor = images;\n","    break;\n","    # Note: input_tensor can be a batch tensor with several images!\n","cam = GradCAM(model=model, target_layers=target_layers)\n"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T13:07:18.761825Z","iopub.status.busy":"2024-04-05T13:07:18.761161Z","iopub.status.idle":"2024-04-05T13:07:19.493876Z","shell.execute_reply":"2024-04-05T13:07:19.492417Z","shell.execute_reply.started":"2024-04-05T13:07:18.761792Z"},"trusted":true},"outputs":[{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 20.12 MiB is free. Process 2427 has 15.87 GiB memory in use. Of the allocated memory 15.50 GiB is allocated by PyTorch, and 57.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m targets \u001b[38;5;241m=\u001b[39m [ClassifierOutputTarget(\u001b[38;5;241m0\u001b[39m)]\n\u001b[0;32m----> 2\u001b[0m grayscale_cam \u001b[38;5;241m=\u001b[39m \u001b[43mcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:192\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_augmentation_smoothing(\n\u001b[1;32m    190\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:83\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_input_gradient:\n\u001b[1;32m     80\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mVariable(input_tensor,\n\u001b[1;32m     81\u001b[0m                                            requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     target_categories \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(outputs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_grad_cam/activations_and_gradients.py:42\u001b[0m, in \u001b[0;36mActivationsAndGradients.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradients \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 30\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m     28\u001b[0m     o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres16[i](o)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres32)):\n\u001b[0;32m---> 30\u001b[0m     o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mres32\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres64)):\n\u001b[1;32m     32\u001b[0m     o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres64[i](o);\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mResNetBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m): \u001b[38;5;66;03m#this means we have to perform 1x1 convolutions\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1x1(x); \n\u001b[0;32m---> 24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1x1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m; \u001b[38;5;66;03m#Applying the 1x1 convolutions to maintain the size.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m o \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m x; \u001b[38;5;66;03m#inplace addition.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m o \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(o); \u001b[38;5;66;03m#the second layer output completed here.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 20.12 MiB is free. Process 2427 has 15.87 GiB memory in use. Of the allocated memory 15.50 GiB is allocated by PyTorch, and 57.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["targets = [ClassifierOutputTarget(0)]\n","grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T12:59:23.907039Z","iopub.status.busy":"2024-04-05T12:59:23.906317Z","iopub.status.idle":"2024-04-05T12:59:51.019965Z","shell.execute_reply":"2024-04-05T12:59:51.018910Z","shell.execute_reply.started":"2024-04-05T12:59:23.906986Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting grad-cam\n","  Downloading grad-cam-1.5.0.tar.gz (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from grad-cam) (1.26.4)\n","Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from grad-cam) (9.5.0)\n","Requirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from grad-cam) (2.1.2)\n","Requirement already satisfied: torchvision>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from grad-cam) (0.16.2)\n","Collecting ttach (from grad-cam)\n","  Downloading ttach-0.0.3-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from grad-cam) (4.66.1)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from grad-cam) (4.9.0.80)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from grad-cam) (3.7.5)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from grad-cam) (1.2.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->grad-cam) (2024.3.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8.2->grad-cam) (2.31.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam) (21.3)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->grad-cam) (2.9.0.post0)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->grad-cam) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->grad-cam) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->grad-cam) (3.2.0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7.1->grad-cam) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8.2->grad-cam) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8.2->grad-cam) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8.2->grad-cam) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8.2->grad-cam) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7.1->grad-cam) (1.3.0)\n","Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n","Building wheels for collected packages: grad-cam\n","  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for grad-cam: filename=grad_cam-1.5.0-py3-none-any.whl size=38071 sha256=4d7f87c02ea3dfda8a790dafde937813d321ac0dbd953549f558356b2afcdc55\n","  Stored in directory: /root/.cache/pip/wheels/5b/e5/3d/8548241d5cffe53ad1476c566a61ad9bf09cc61a9430f09726\n","Successfully built grad-cam\n","Installing collected packages: ttach, grad-cam\n","Successfully installed grad-cam-1.5.0 ttach-0.0.3\n"]}],"source":["\n","# Construct the CAM object once, and then re-use it on many images:\n","\n","# You can also use it within a with statement, to make sure it is freed,\n","# In case you need to re-create it inside an outer loop:\n","# with GradCAM(model=model, target_layers=target_layers) as cam:\n","#   ...\n","\n","# We have to specify the target we want to generate\n","# the Class Activation Maps for.\n","# If targets is None, the highest scoring category\n","# will be used for every image in the batch.\n","# Here we use ClassifierOutputTarget, but you can define your own custom targets\n","# That are, for example, combinations of categories, or specific outputs in a non standard model.\n","\n","\n","# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n","\n","# In this example grayscale_cam has only one image in the batch:\n","grayscale_cam = grayscale_cam[0, :]\n","visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n","\n","# You can also get the model outputs without having to re-inference\n","model_outputs = cam.outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-02T06:09:33.672092Z","iopub.status.idle":"2024-04-02T06:09:33.672457Z","shell.execute_reply":"2024-04-02T06:09:33.672311Z","shell.execute_reply.started":"2024-04-02T06:09:33.672297Z"},"trusted":true},"outputs":[],"source":["# with open(\"/kaggle/input/checkpoints-part1a/traindata.pickle\", \"rb\") as file:\n","#     traindata = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-02T06:09:33.673412Z","iopub.status.idle":"2024-04-02T06:09:33.673701Z","shell.execute_reply":"2024-04-02T06:09:33.673566Z","shell.execute_reply.started":"2024-04-02T06:09:33.673554Z"},"trusted":true},"outputs":[],"source":["with open('traindata.pickle', 'wb') as file:\n","    pickle.dump(traindata, file);\n","store_checkpoint(model, optimizer, 51, 2, \"final_chkpnt.pth\"); #stores the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-02T06:09:33.674852Z","iopub.status.idle":"2024-04-02T06:09:33.675228Z","shell.execute_reply":"2024-04-02T06:09:33.675050Z","shell.execute_reply.started":"2024-04-02T06:09:33.675005Z"},"trusted":true},"outputs":[],"source":["print(\"VAL: \", check_eval(Val_loader, model));\n","print(\"TEST: \", check_eval(Test_loader, model));\n","print(\"TRAIN: \", check_eval(Train_loader, model));"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-01T23:19:12.772957Z","iopub.status.idle":"2024-04-01T23:19:12.773310Z","shell.execute_reply":"2024-04-01T23:19:12.773165Z","shell.execute_reply.started":"2024-04-01T23:19:12.773150Z"},"trusted":true},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","# traindata['val'] = [];\n","# for i in range(50):\n","#     print(i, \" doing     \\r\")\n","#     file_name = '/kaggle/input/checkpoints-part1a/checkpoint_epoch' + str(i) + '.pth'\n","#     load_checkpoint(model, optimizer, file_name)\n","#     valacc, valmac, valmic = check_eval(Val_loader, model);\n","#     traindata['val'].append((valacc, valmac, valmic));\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-01T23:19:12.774529Z","iopub.status.idle":"2024-04-01T23:19:12.774844Z","shell.execute_reply":"2024-04-01T23:19:12.774697Z","shell.execute_reply.started":"2024-04-01T23:19:12.774685Z"},"trusted":true},"outputs":[],"source":["def plot_epoch(data, name):\n","    index = list(range(1, len(data) + 1))\n","    accuracy, micro_f1, macro_f1 = zip(*data)\n","\n","    # Plotting\n","    fig, ax = plt.subplots(figsize=(10, 8))\n","\n","    ax.plot(index, accuracy, marker='o', label='Accuracy')\n","    ax.plot(index, micro_f1, marker='s', label='Micro F1')\n","    ax.plot(index, macro_f1, marker='^', label='Macro F1')\n","    ax.set_title('Performance Metrics for ' + name)\n","    ax.set_xlabel('List Index')\n","    ax.set_ylabel('Score')\n","    ax.set_xticks(index)\n","    ax.legend()\n","    ax.grid(True)\n","    plt.tight_layout()\n","    plt.savefig(name) #The fig should be saved before doing plt.show().\n","    plt.show()\n","plot_epoch(traindata['train'], 'train plot');\n","plot_epoch(traindata['val'], 'val plot');\n","plot_epoch(traindata['test'], 'test plot');\n","plot_epoch(traindata['loss'], 'train loss');"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4714016,"sourceId":8004432,"sourceType":"datasetVersion"},{"datasetId":4738233,"sourceId":8037319,"sourceType":"datasetVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
